---
title: "Homework 5 - Optimization"
author: "Sophie Shan"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1:  Simple Univariate Optimization

Use Newton-Raphson to maximize the following function:  $$f(x) = 7\log(x) + 3\log(1-x).$$  Fill in the R functions below pertaining to $f(x)$, $f'(x)$ and $f''(x)$.  Utilize the values below to initialize and run the algorithm (analogous to how they were defined in class).  Repeat the method for several starting values for $x$ given below, picking the best $\hat{x}$ in terms of the final objective function value for each. Make sure you print your final value for $\hat{x}$

```{r}
# f(x)
f = function(x){
  ## solution
  func = 7 * log(x) + 3 * log(1-x)
 
  ## end solution
  return(func)
}

# first derivative
f1 = function(x){
  ## solution
  first = (7/x) - (3/(1-x))

  ## end solution
  return(first)
}

# second derivative
f2 = function(x){
  ## solution
  second = - (7/(x^2)) - (3/(1-x)^2)
  
  ## end solution
  return(second)
}

# to start the model
tol = 10^-4
x_vec = c(0.01, 0.5, 0.99)
maxit = 50
iter = 0
eps = Inf

## solution
for(i in 1:3){
  
  print(paste0("We're on :", x_vec[i]))
  
  while (eps > tol & iter < maxit) {
  # save the previous value
  x0 = x_vec[i]
  
  # calculate h, the increment
  h = -f1(x_vec[i]) / f2(x_vec[i])
  
  # update x
  x_vec[i] = x_vec[i] + h
  
  # calculate the diff in x
  eps  = abs(x_vec[i] - x0)
  
  # update the iteration number
  iter = iter + 1
  if (iter == maxit)
    warning("Iteration limit reached without convergence")
  
  # print out info to keep track
  cat(sprintf("Iter: %d x: %f h: %f eps:%f\n", iter, x_vec[i], h, eps))
  }
  
  #reset the eps and iter
  iter = 0
  eps = Inf

}


## end solution
print(paste0("Our final solution is :", x_vec))
```

Bonus:  $f(x)$ pertains to the likelihood/PDF for which distribution(s)?  Two answers are possible.  Given this, what would be a closed form estimate for $\hat{x}$?

f(x) pertains to the log-likelihood of a binomial distribution.

$$l(x| k, n) = k log(x) + (n-k) log(1-x)$$ where k = 7 and n = 10. 

A closed formed estimate for $\hat{x}$ would be the MLE. We take the derivative of the log-likelihood which is $\frac{dl(x)}{x} = \frac{7}{x} - \frac{3}{1-x} $ and set to 0 and solve for x. 

We get $\hat{x} = \frac{7}{10}$. 

# Question 2:  Not So Simple Univariate Optimization

Repeat question 1 for the function below, using the same template for your answer.  Choose a range of starting values between 0 and 5 and report the best $\hat{x}$ based on the final objective function value:

$$f(x) = 1.95 - e^{-2/x} - 2e^{-x^4}.$$

```{r}
# f(x)
f = function(x){
  ## solution
  func = 1.95 - exp(-(2/x)) - 2*exp(-(x^4))
  
  ## end solution
  return(func)
}

# first derivative
f1 = function(x){
  ## solution
  first = 8*(x^3)*exp(-x^4) - (2*exp(-(2/x)))/(x^2) 

  ## end solution
  return(first)
}

# second derivative
f2 = function(x){
  ## solution
  second = -32*(x^6)*exp(-x^4) + 24*x^2*exp(-x^4) + (4*exp(-2/x))/(x^3) - (4*exp(-2/x))/(x^4)

  ## end solution
  return(second)
}

# to start the model
set.seed(1)

tol = 10^-4
x_vec = sort(runif(10, 0,5))
maxit = 50
iter = 0
eps = Inf

#Initialize a list to store results
results = list()


## solution
for(i in 1:length(x_vec)){
  
  print(paste0("We're on :", x_vec[i]))
  
  while (eps > tol & iter < maxit) {
  # save the previous value
  x0 = x_vec[i]
  
  # calculate h, the increment
  h = -f1(x_vec[i]) / f2(x_vec[i])
  
  # update x
  x_vec[i] = x_vec[i] + h
  
  # update the likelihood
  L = f(x_vec[i])
  
  # calculate the diff in x
  eps  = abs(x_vec[i] - x0)
  
  # update the iteration number
  iter = iter + 1
  if (iter == maxit)
    warning("Iteration limit reached without convergence")
  
  # print out info to keep track
#  cat(sprintf("Iter: %d x: %f h: %f eps:%f\n", iter, x_vec[i], h, eps))
  }
  
  #save the last one 
  results[[i]] <- list(iter=iter, x=x_vec[i], L=L, h=h, eps=eps)
  
  #reset the eps and iter
  iter = 0
  eps = Inf
  
}

#bind together the results
do.call(rbind, results)

#SOLUTION: x = 1.470358 

## end solution

```

What does this say about the stability of NR, especially in this case?  Plot the second derivative of this function and comment on why or why this is supports your observations.

NR is NOT stable since whether you converge or not depends on where you initialize the function. The
second derivative is flat after "2" which means that if you initialize anywhere there, 
you have no way of converging to the maximum using NR. 
```{r}
## solution for plot
set.seed(1)
x <- sort(runif(300, 0,5))
y <- f2(x)

plot(x,y)

## end solution
```


## Multivariate optimization:  Zero-inflated Poisson 

Following a chemical spill from a local industrial plant, the city government is attempting to characterize the impact of the spill on recreational fishing at a nearby lake.  Over the course of one month following the spill, park rangers asked each adult leaving the park how many fish they had caught that day. At the end of the month they had collected responses for 4,075 individuals, where the number of fish caught for each individual is summarized in the table below:

```{r, echo=F}
library(knitr)
kable(matrix(c(3062, 587, 284, 103, 33, 4, 2), nrow = 1, byrow = T),col.names = as.character(0:6),format = "html", table.attr = "style='width:30%;'",)
```

Based on an initial examination of this distribution, there appears to be many more zeros than expected.  Upon speaking to the park rangers, they were embarrassed to state that after recording the number of fish each adult had caught, they did not ask or record whether each adult had gone to the park with the intention of fishing in the first place.  

The city statistician surmised that the observed distribution in the table above may be resulting from a mixture of two populations of subjects.  The first population pertains to the subset of visitors that arrived without any intention of fishing (exactly zero fish caught by each member of this population). The second population pertains to the set of visitors that arrived with the intention of fishing (0 or more fish potentially caught in this population).  Therefore, if we fit a standard Poisson model to this data, the estimate for $\lambda$ would be biased downwards.  

To account for the excess zeros in the observed data, the statistician decided to fit a zero-inflated Poisson model.  To simplify things the log likelihood is given below, and we utilized the tabulated values from the table above in place of individual observations: 

$$ 
\mathcal{l}(\boldsymbol{\theta}) = n_0\log(\pi + (1-\pi)e^{-\lambda}) + (N-n_0)[\log(1-\pi) - \lambda] + \sum_{i=1}^{\infty}in_i\log(\lambda)
$$

where $\boldsymbol{\theta} = (\pi, \lambda)$, $n_i$ pertains to the number of individuals that caught $i$ fish ($n_i=0$ for $i>6$ here), $N = 4075$, $\pi$ is the probability that an individual did not show up to fish, and $\lambda$ is the mean number of fish caught by those individuals that intended to fish at the park.  From this log-likelihood, we can see that for the individuals that caught 0 fish, we assume those observations come from a mixture of individuals that did not show up to fish ($\pi$ proportion of the time) and those that showed up to fish but did not catch anything ($1-\pi$ proportion of the time with 0 for their catch count).  The remaining individuals that caught more than zero fish are also in the log likelihood and are similarly weighted by $(1-\pi)$. 


Lets go ahead and fit this model with Newton-Raphson.  Similar to the previous problems, fill in the code below to fit this model.  Then, fill in the function for the log likelihood and its 1st derivative $$\left(\frac{\partial\mathcal{l}(\boldsymbol{\theta})}{\partial\pi}, \frac{\partial\mathcal{l}(\boldsymbol{\theta})}{\partial\lambda}\right)$$ and second derivative matrix 

$$\left[\begin{array}
{rr}
\frac{\partial^2\mathcal{l}(\boldsymbol{\theta})}{\partial\pi^2} & \frac{\partial^2\mathcal{l}(\boldsymbol{\theta})}{\partial\pi \partial\lambda} \\
\frac{\partial^2\mathcal{l}(\boldsymbol{\theta})}{\partial\lambda \partial\pi } & \frac{\partial^2\mathcal{l}(\boldsymbol{\theta})}{\partial\lambda^2} 
\end{array}\right] $$  

Given the estimates, interpret the results. Accounting for the excess zeros, on average how many fish were caught among those who intended to go fishing?  What proportion of subjects did not even consider to fish?


```{r}
# f(x)
logLik = function(theta, y, ny){
  
  ## solution
  n_0 = ny[1]
  pi = theta[1]
  lamda = theta[2]
  
  p1 = n_0 * log(pi + (1 - pi)*exp(-lamda))
  p2 = (4075 - n_0) * (log(1 - pi) - lamda)
  p3 = log(lamda)*sum(y * ny)
  
  p = p1 + p2 + p3
  
  # returns scalar
  p
  
  ## end solution
}

# first derivative
f1 = function(theta, y, ny){
  ## solution
  n_0 = ny[1]
  pi = theta[1]
  lamda = theta[2]
  
  #pi
  pi_1_top = n_0 * (1 - exp(-lamda))
  pi_1_bottom = pi + exp(-lamda) * (1 - pi)
  pi_2_top = 4075 - n_0
  pi_2_bottom = 1 - pi
  
  first_pi = (pi_1_top/pi_1_bottom) - (pi_2_top/pi_2_bottom)
  
  #lamda 
  lamda_1_top = n_0 * (1 - pi) * exp(-lamda)
  lamda_1_bottom = (1 - pi) * exp(-lamda) + pi
  lamda_2 = n_0 - 4075
  lamda_3 = (1/lamda) * sum(y * ny)
  
  first_lamda = -(lamda_1_top/lamda_1_bottom) + lamda_2 + lamda_3
    
  # returns 2x1 vector
  return(matrix(c(first_pi, first_lamda)))
  
  ## end solution
}

# second derivative
f2 = function(theta,y, ny){
  ## solution
  n_0 = ny[1]
  pi = theta[1]
  lamda = theta[2]
  
  #pi
  pi_1_top = n_0 * (1 - exp(-lamda))^2
  pi_1_bottom = (pi + exp(-lamda) * (1-pi))^2
  pi_2_top = 4075 - n_0
  pi_2_bottom = (1-pi)^2
  
  second_pi = -(pi_1_top/pi_1_bottom) - (pi_2_top/pi_2_bottom)
  
  
  #lamda
  lamda_1_top = n_0 * (pi - 1) * pi * exp(lamda)
  lamda_1_bottom = (pi * exp(lamda) - pi + 1)^2
  lamda_2 = -(1/lamda^2)*sum(y * ny)
  
  second_lamda = -(lamda_1_top/lamda_1_bottom) + lamda_2
  
  #pi_lamda 
  pi_lamda_top = n_0 * exp(-lamda)
  pi_lamda_bottom = (pi*exp(lamda) - pi + 1)^2
  
  second_pi_lamda = pi_lamda_top/pi_lamda_bottom
  
  
  # returns 2x2 matrix
  return(matrix(c(second_pi, second_pi_lamda,
                  second_pi_lamda, second_lamda),
                nrow = 2, 
                ncol = 2))
  
  ## end solution
}

# data 
y = 0:6
ny = c(3062, 587, 284, 103, 33, 4, 2)

# to start the model
tol = 10^-4
theta = c(
          ny[y==0]/sum(ny), # initial value for pi, prop of total 0's
          sum(ny*y)/sum(ny) # intial value for lambda, mean of y's
          )
maxit = 50
iter = 0
eps = Inf

## solution
while (eps > tol & iter < maxit) {
  # save the previous value
  theta0 = theta
  
  # calculate h, the increment
  h = solve(f2(theta, y, ny)) %*% f1(theta, y, ny) 
  
  # update x
  theta = theta - h
  
  # calculate the diff in x
  eps  = max(abs(theta - theta0))
  
  # update the iteration number
  iter = iter + 1
  if (iter == maxit)
    warning("Iteration limit reached without convergence")
  
  # print out info to keep track
  cat(sprintf("Iter: %d pi: %f lamda: %f h_top: %f h_bottom: %f eps:%f\n", 
              iter, theta[1], theta[2], h[1], h[2], eps))
  }


#SOLUTION: ~ 1 fish were caught among those who intended to go fishing. About 60% of 
#subjects did not even consider to fish.
## end solution
```











